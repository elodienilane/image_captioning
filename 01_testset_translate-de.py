# -*- coding: utf-8 -*-
"""Tests_translate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wBKukHUryWeEOa-EaXtk-BYasZyjGNYj

Import ParlAI
"""

#parlai
#!git clone https://github.com/facebookresearch/ParlAI.git
#!cd ParlAI; python setup.py develop

#!python -m parlai.scripts.display_data -t personality_captions

#!python projects/personality_captions/interactive.py -mf models:personality_captions/transresnet/model

"""Load file in Google Drive"""

#fichier à importer chez moi dans : C:\Users\ElodieNilane\Documents\IA\ProjectTest\ParlAI\data\personality_captions
import codecs, json


"""Translate text unit"""

#googletrans : https://pypi.org/project/googletrans/, https://py-googletrans.readthedocs.io/en/latest/

#!pip install googletrans
#!pip install translate
#!pip install google_translate
#!pip install pandas
from googletrans import Translator as GoogleTranslator

#Avec la librairie de Google : IP bloquée après trop de requête
def translate_google(text = 'hello world', source_lng = 'en', dest_lng = 'ja'):
  translator = GoogleTranslator(service_urls=[
      'translate.google.com',
      'translate.google.co.kr',
      'translate.google.fr',
      'translate.google.ch',], proxies={'http': '37.26.136.181:40076', 'http':'138.197.145.103:8080', 'http': '165.227.215.71:8080', 'http':'88.198.24.108:8080'})
  result = translator.translate(text, dest=dest_lng)
  return result

"""Translate file"""

from os import chdir
wd=r'D:\Documents\IA\ProjectTest\ParlAI\data\personality_captions\Full_program_German'
chdir(wd)

#Parse file to save only text to translate
def parse_file(source_file_path, dest_file_path):
  data = ""
  total = 0
  
  dest = open(dest_file_path, "w+")

  with codecs.open(source_file_path, 'r', 'utf8') as data_file:    
      data = json.load(data_file)
      for i in data:
        for key in i:
          if key=="comment":
              dest.write(i[key]+u"\n")
              total = total + 1
  print('Total parsed:%s', total)


#train file to jap
file_type = 'train'
language = 'de'
original_file_name = 'Original_Data\\' + file_type + ".json"
parsed_file_name =  'Translated_Data\\' + file_type + '_parsed_' + language + '.txt'
translated_file_name =  'Translated_Data\\' + file_type + '_translated_' + language + '.txt'

parse_file(original_file_name, parsed_file_name)
print("Parsed file "+ original_file_name +" to " + parsed_file_name)


"""
Translate file to destination language
"""

#Get phrases and translate them to destination language


from itertools import islice
import time

def next_n_lines(file_opened, N):
    return [x.strip() for x in islice(file_opened, N)]

n = 20
num_lines = sum(1 for line in open(parsed_file_name))
read_lines = 0
print(num_lines)

transl_dest = open(translated_file_name, "a", encoding="utf-8")
already_done = sum(1 for line in open(translated_file_name, encoding="utf-8"))

print(already_done)
attempt_count = 0

with open(parsed_file_name, 'r') as sample:
  #dump the first lines which were already parsed
  line = next_n_lines(sample, already_done)
  read_lines = already_done
  while read_lines < num_lines :
    lines_read = next_n_lines(sample, n)
    read_lines += n
    print("Read lines : " + str(read_lines))
    while True:
      try:
        trans = translate_google(lines_read, 'en', language)
        attempt_count = 0
      except ValueError:
        attempt_count += 1

        print("Attempt failed " + str(attempt_count))
        time.sleep(1800)
        continue
      break
    
    for translation in trans:
      print(translation.origin, ' -> ', translation.text)
      transl_dest.write(translation.text+u"\n")

    time.sleep(10)
transl_dest.close()



"""
Reconstruct json file in destination language
"""


"""
******************************************************************TODO
OPEN FILE AND SAVE AS UTF-8"""

#import json 
import pandas as pd

# Constants for file names
dest_file = 'Translated_Data\\' + file_type + '_' + language + '.json'

# Importing the dataset
translated_comments = pd.read_table(translated_file_name, names=['comment'])
orig_file = pd.read_json(original_file_name)


#Parse file to save only text to translate
def translate_file(json_file_path, comments_file_path, dest_file_path):
  data = ""
  j = 0
  total = 0

  with codecs.open(json_file_path, 'r', 'utf8') as comments_file:    
      data = json.load(comments_file)
      for json_object in data:
        for key in json_object:
          if key=="comment":
            json_object[key] = translated_comments.iloc[total, 0]
            total = total + 1
        j+=1
      print(total)

      with codecs.open(dest_file_path, 'w', 'utf8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

translate_file(original_file_name, translated_file_name, dest_file)
